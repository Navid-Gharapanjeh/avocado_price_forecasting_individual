{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avocado Price Prediction - CRISP-DM Phase\n",
    "\n",
    "## Addressing Teacher's Feedback\n",
    "\n",
    "Hopefully There is a chance for retake\n",
    "On my previous version I got 4.4 ( consider failed ) also received feedback highlighting several areas for improvement:\n",
    "\n",
    "1. **Lack of accompanying text, explanations, and reasoning**\n",
    "2. **No clear definition of CRISP-DM phase objectives**\n",
    "3. **Insufficient justification for selecting critical criteria**\n",
    "4. **No reflection on data visualization findings**\n",
    "5. **Missing weather data exploration and correlation analysis**\n",
    "\n",
    "These enhanced notebooks directly address this feedback by:\n",
    "\n",
    "1. **Adding detailed explanations** for every code block\n",
    "2. **Clearly defining objectives** for each CRISP-DM phase\n",
    "3. **Providing justifications** for all decisions with references to research\n",
    "4. **Reflecting on visualizations** and explicitly linking findings to next steps\n",
    "5. **Thoroughly exploring weather data** and its correlation with avocado prices\n",
    "\n",
    "---\n",
    "\n",
    "### Project Overview\n",
    "\n",
    "This project aims to develop a machine learning model to predict Hass avocado prices in various regions of the United States. The prediction model will utilize historical data including sales volumes, types of avocados, regional information, and weather data to forecast future prices.\n",
    "\n",
    "## 1. Business Understanding\n",
    "\n",
    "In this notebook, I define the business context, objectives, success criteria, and constraints for the avocado price forecasting project. The Business Understanding phase is the critical first step in the CRISP-DM methodology, establishing a clear foundation for all subsequent phases. In this section, we'll cover:\n",
    "- Market overview and trends\n",
    "- Business objectives and research questions\n",
    "- Success criteria and metrics\n",
    "- Project constraints and resources\n",
    "\n",
    "**What is CRISP-DM?** The CRoss-Industry Standard Process for Data Mining (CRISP-DM) is a proven, robust process model for guiding data mining and machine learning projects. It consists of six phases: Business Understanding, Data Understanding, Data Preparation, Modeling, Evaluation, and Deployment.\n",
    "\n",
    "**Why is Business Understanding important?** This phase ensures that the project addresses actual business needs rather than being a purely technical exercise. By clearly defining objectives and success criteria upfront, we can ensure that our modeling efforts remain focused on delivering tangible business value.\n",
    "\n",
    "## The Avocado Boom: A Decade of Growth and Consumption Surge\n",
    "\n",
    "The avocado has transitioned from a niche product to a global dietary staple, witnessing a remarkable surge in consumption over the past ten years. This growth is driven by increasing awareness of its health benefits, culinary versatility, and expanded availability.\n",
    "\n",
    "## Global Consumption Trends\n",
    "\n",
    "* **Significant Global Expansion:** The global avocado industry has experienced substantial growth, with production and trade expanding rapidly. Global avocado production increased at a compound annual growth rate of approximately 7% over the past decade, reaching over 8.4 million metric tons in 2022.\n",
    "* **Regional Growth:**\n",
    "    * **North America:** Consumption has nearly doubled in the last ten years. The U.S. remains a major consumer, with a significant portion of its avocados imported from Mexico.\n",
    "    * **Europe:** Avocado consumption in Europe has also seen a dramatic increase, nearly tripling over the past decade. Countries like Germany, the UK, and Spain have shown particularly strong growth.\n",
    "    * **Latin America:** There has been very strong growth in Latin American countries consumptions also.\n",
    "    * **Global trade:** Mexico is the world's largest exporter of Avocados.\n",
    "\n",
    "## US Consumption Statistics\n",
    "\n",
    "* **Dramatic Increase:** In the United States, avocado consumption has seen a particularly dramatic increase. According to the Hass Avocado Board, U.S. per capita consumption of avocados has increased from 3.5 pounds in 2006 to 8.5 pounds in 2020.\n",
    "* **Percentage Growth:** This represents a 143% growth in demand.\n",
    "\n",
    "## Key Drivers\n",
    "\n",
    "* Increased popularity of avocados in various cuisines.\n",
    "* Growing awareness of the health benefits of avocados.\n",
    "* Expanded availability of avocados in supermarkets and restaurants.\n",
    "\n",
    "## References\n",
    "\n",
    "* Global avocado industry: a decade of growth - Fresh Fruit Portal: [link](https://www.freshfruitportal.com/news/2023/05/11/global-avocado-industry-a-decade-of-growth/)\n",
    "* Hass Avocado Board, 2022.\n",
    "\n",
    "This growth in popularity has been accompanied by substantial price volatility due to several factors:\n",
    "- **Seasonal production cycles** affecting supply\n",
    "- **Weather events** impacting harvests in major producing regions\n",
    "- **Regional demand variations** across different US markets\n",
    "- **Growing preference for organic produce** affecting market segmentation\n",
    "\n",
    "### Business Value of Price Prediction\n",
    "Accurate price prediction can deliver substantial value to various stakeholders in the supply chain:\n",
    "\n",
    "- **Retailers**: \n",
    "  - Optimize inventory management to reduce waste (estimated 12% reduction potential)\n",
    "  - Develop data-driven pricing strategies to maximize profitability while maintaining competitiveness\n",
    "  - Plan promotional activities during periods of favorable pricing\n",
    "  \n",
    "- **Farmers**: \n",
    "  - Make informed decisions about harvesting timing to maximize returns\n",
    "  - Plan production based on predicted market conditions\n",
    "  - Optimize resource allocation for different avocado varieties\n",
    "  \n",
    "- **Consumers**: \n",
    "  - Benefit from more stable pricing through improved market efficiency\n",
    "  - Make more informed purchasing decisions based on price forecasts\n",
    "\n",
    "\n",
    "## 1.1 Business Objectives and Research Questions\n",
    "\n",
    "### Primary Business Objectives:\n",
    "1. **Develop a reliable price prediction model for Hass avocados**: Create a forecasting model that accurately predicts avocado prices across different regions and market segments to support decision-making for stakeholders.\n",
    "\n",
    "2. **Identify key factors influencing avocado prices**: Determine the most significant variables affecting price variations to provide actionable insights for stakeholders.\n",
    "\n",
    "### Research Questions:\n",
    "To guide our analysis and modeling efforts, I have formulated the following research questions:\n",
    "\n",
    "1. **How accurately can I predict avocado prices using historical data?**\n",
    "   - *Importance*: Establishes the feasibility of the core objective and defines performance benchmarks for the prediction model.\n",
    "   - *Approach*: Evaluate different forecasting techniques and quantify prediction accuracy through appropriate metrics.\n",
    "\n",
    "2. **What features have the strongest influence on avocado prices?**\n",
    "   - *Importance*: Identifies key price drivers that stakeholders can monitor and potentially influence.\n",
    "   - *Approach*: Implement feature importance analysis across different model types to identify consistently significant predictors.\n",
    "\n",
    "3. **How do regional differences affect pricing patterns?**\n",
    "   - *Importance*: Regional market dynamics significantly impact local pricing strategies and distribution decisions.\n",
    "   - *Approach*: Analyze price variations across regions and identify region-specific trends and factors.\n",
    "\n",
    "4. **What is the impact of organic vs. conventional classification on prices?**\n",
    "   - *Importance*: The premium for organic produce varies over time and by region, affecting production and purchasing decisions.\n",
    "   - *Approach*: Compare price trends between organic and conventional avocados and identify factors affecting the price differential.\n",
    "\n",
    "5. **How do seasonal patterns affect avocado prices?**\n",
    "   - *Importance*: Seasonality is a critical factor in agricultural commodities that affects both supply and demand.\n",
    "   - *Approach*: Conduct time series decomposition to isolate seasonal components and analyze their consistency across years.\n",
    "   \n",
    "6. **How does weather influence avocado prices across different regions?**\n",
    "   - *Importance*: Weather conditions affect both production and consumption patterns in complex ways.\n",
    "   - *Approach*: Analyze correlations between weather variables and prices while controlling for other factors.\n",
    "\n",
    "## 2. Business Success Criteria\n",
    "\n",
    "The success of this project will be primarily evaluated based on the accuracy of predicting avocado prices, differentiated by organic versus conventional types, across distinct US regions. \n",
    "\n",
    "### 2.1 Technical Success Criteria:\n",
    "Based on my research, it would be hard to say what score is good or excellent for each model, and the numbers here are based on my general research. For instance, I'm not sure that R² ≥ 0.7 in linear regression is good for this context or not. I think there would be a formula or something to identify what is an acceptable score for each case.\n",
    "\n",
    "#### Statistical/Time Series Models:\n",
    "\n",
    "* **R² ≥ 0.7 for Linear Regression:**\n",
    "    * *Justification:* Represents a good fit for linear models in agricultural price forecasting.\n",
    "    * *Current Status:* **Not achieved (0.58)**, indicating linear models are insufficient for this task's complexity.\n",
    "\n",
    "#### Machine Learning Models:\n",
    "\n",
    "* **R² ≥ 0.9 for Random Forest:**\n",
    "    * *Justification:* Ensemble methods like Random Forest are expected to achieve high accuracy for complex, non-linear relationships.\n",
    "    * *Current Status:* **Achieved (0.88)**, demonstrating strong predictive performance.\n",
    "    * *Note:* While 0.9 is excellent, 0.88 is still very good and could be considered acceptable depending on the specific business requirements.\n",
    "* **MAE < 0.1:**\n",
    "    * *Justification:* Represents a prediction error of less than 10% on average avocado prices ($1-2), considered actionable for business planning.\n",
    "    * *Current Status:* **Achieved (0.07)**, indicating good absolute prediction accuracy.\n",
    "* **RMSE < 0.15:**\n",
    "    * *Justification:* Captures and penalizes larger prediction errors, crucial for preventing costly business decisions based on outliers.\n",
    "    * *Current Status:* **Achieved (0.10)**, showing good performance with limited large errors.\n",
    "    * *Note:* It is important to remember that RMSE is more sensitive to outliers than MAE.\n",
    "     \n",
    "\n",
    "## 1.3 Situation Assessment\n",
    "\n",
    "A comprehensive assessment of the project context, resources, and constraints is essential for establishing realistic expectations and designing an appropriate approach.\n",
    "\n",
    "#### Data Resources:\n",
    "- **Avocado Price and Volume Data**: \n",
    "  - Weekly retail scan data for National retail volume and price (2015-2023)\n",
    "  - Data from multiple retail channels (grocery, mass, club, drug, dollar, military)\n",
    "  - Sourced from Kaggle datasets and includes historical trends\n",
    "  - Contains details on both conventional and organic avocados\n",
    "  \n",
    "- **Product Information**:\n",
    "  - PLU codes identifying different avocado types and sizes\n",
    "  - Regional market classification\n",
    "  \n",
    "- **Weather Data**:\n",
    "  - Historical temperature data for US regions corresponding to avocado markets\n",
    "  - Retrieved from Meteostat API\n",
    "\n",
    "\n",
    "#### Technologies Overview\n",
    "\n",
    "##### Project Structure and Management\n",
    "- **Cookie Cutter Data Science** - Standardized project structure template\n",
    "- **Python 3.12** - Primary programming language\n",
    "- **Jupyter Notebooks/JupyterLab** - Interactive development\n",
    "- **pandas** - Data manipulation and analysis\n",
    "- **numpy** - Numerical computing and array operations\n",
    "- **matplotlib** - Basic plotting capabilities\n",
    "- **seaborn** - Advanced statistical data visualization\n",
    "- **scikit-learn**\n",
    "  - Data preprocessing (StandardScaler, LabelEncoder, SimpleImputer)\n",
    "  - Model selection (train_test_split, cross_val_score)\n",
    "  - ML models (GradientBoostingRegressor, RandomForestRegressor, LinearRegression)\n",
    "  - Metrics (mean_squared_error, r2_score, mean_absolute_error)\n",
    "  - Pipeline workflow automation\n",
    "- **XGBoost** - Advanced gradient boosting implementation\n",
    "\n",
    "\n",
    "### Constraints and Assumptions:\n",
    "\n",
    "#### 1. Data Constraints:\n",
    "   - **Limited to Hass avocados only**: \n",
    "     - *Impact*: Cannot generalize findings to other avocado varieties\n",
    "     - *Mitigation*: Focus analysis specifically on Hass market dynamics and clearly communicate this limitation\n",
    "     \n",
    "   - **Weekly granularity of data**: \n",
    "     - *Impact*: Unable to capture daily price fluctuations or intra-week patterns\n",
    "     - *Mitigation*: Focus on week-to-week trends and longer-term forecasting horizons\n",
    "     \n",
    "   - **Historical data might not capture recent market changes**: \n",
    "     - *Impact*: Model may not account for emerging trends or structural market shifts\n",
    "     - *Mitigation*: Implement a validation approach using the most recent data and monitor model drift\n",
    "     \n",
    "   - **Weather data limitations**: \n",
    "     - *Impact*: Weather data for regions is averaged and may not reflect microclimate variations\n",
    "     - *Mitigation*: Use weather as a general indicator rather than a precise predictor\n",
    "\n",
    "#### 2. Technical Constraints:\n",
    "   - **No deep learning techniques allowed**: \n",
    "\n",
    "\n",
    "## 5. Project Roadmap\n",
    "\n",
    "Based on the CRISP-DM methodology, the project will proceed through the following phases:\n",
    "\n",
    "### 1. Business Understanding\n",
    "- Define objectives and success criteria\n",
    "- Assess situation and resources\n",
    "- Establish project plan\n",
    "\n",
    "### 2. Data Understanding\n",
    "- Explore avocado price and volume data\n",
    "- Analyze weather data and its relationship to prices\n",
    "- Identify data quality issues\n",
    "\n",
    "\n",
    "### 3. Data Preparation\n",
    "- Clean and transform raw data\n",
    "- Feature engineering for temporal, categorical, and regional variables\n",
    "- Create training, validation, and test datasets\n",
    "- Document all transformations for reproducibility\n",
    "\n",
    "### 4. Modeling\n",
    "- Develop baseline models (Linear Regression)\n",
    "- Implement advanced models (Random Forest, Gradient Boosting)\n",
    "- Document model architecture and training process\n",
    "\n",
    "### 5. Evaluation\n",
    "- Assess model performance against technical criteria\n",
    "- Evaluate business value of insights\n",
    "- Compare different modeling approaches\n",
    "- Identify strengths and limitations\n",
    "\n",
    "***\n",
    "## 2. Data Understanding\n",
    "\n",
    "The initial phase focused on gaining familiarity with the dataset's structure, content, quality, and statistical properties.\n",
    "\n",
    "#### Data Sources:\n",
    "- Avocado sales and price data from Kaggle (2015-2023) [link](https://www.kaggle.com/datasets/vakhariapujan/avocado-prices-and-sales-volume-2015-2023)\n",
    "- Weather history from Meteostat API [link](https://dev.meteostat.net/api/)\n",
    "\n",
    "#### Avocado Dataset Description\n",
    "\n",
    "The dataset consists of historical weekly data on HASS Avocado prices and sales volume across multiple U.S. markets. It contains 53,415 observations with 12 columns, which we consider it as a **structure** dataset. Below is a table describing each column:\n",
    "\n",
    "| Column Name   | Description |\n",
    "|--------------|------------|\n",
    "| Date (Time)                   | The date of the observation  |\n",
    "| AveragePrice (Target)         | The average price of a single avocado |\n",
    "| TotalVolume  (Volume)         | Total number of avocados sold |\n",
    "| plu4046      (Volume)         | Total number of avocados with PLU 4046 (Small) sold |\n",
    "| plu4225      (Volume)         | Total number of avocados with PLU 4225 (Larg) sold |\n",
    "| plu4770      (Volume)         | Total number of avocados with PLU 4770 (Medium) sold |\n",
    "| TotalBags    (Volume)         | Total Bags Sold |\n",
    "| SmallBags    (Volume)         | Small Bags Sold |\n",
    "| LargBags     (Volume)         | Larg Bags Sold |\n",
    "| XLargBags    (Volume)         | XLarg Bags Sold |\n",
    "| type  (Categorical)           | Organic or Conventional |\n",
    "| region  (Categorical)         | The city or region of the observation |\n",
    "\n",
    "\n",
    "This dataset can be used for various analytical and predictive modeling tasks, including price forecasting and trend analysis.\n",
    "\n",
    "And here is weather dataset structure :\n",
    "\n",
    "| Parameter | Description                                     | Type    |\n",
    "|-----------|-------------------------------------------------|---------|\n",
    "| region    | The city or region of the observation            | String  |\n",
    "| date      | The date string (YYYY-MM-DD)                     | String  |\n",
    "| tavg      | The average air temperature in °C                | Float   |\n",
    "| tmin      | The minimum air temperature in °C                | Float   |\n",
    "| tmax      | The maximum air temperature in °C                | Float   |\n",
    "| prcp      | The daily precipitation total in mm              | Float   |\n",
    "| snow      | The maximum snow depth in mm                     | Integer |\n",
    "| wdir      | The average wind direction in degrees (°)        | Integer |\n",
    "| wspd      | The average wind speed in km/h                   | Float   |\n",
    "| wpgt      | The peak wind gust in km/h                      | Float   |\n",
    "| pres      | The average sea-level air pressure in hPa        | Float   |\n",
    "| tsun      | The daily sunshine total in minutes (m)          | Integer |\n",
    "\n",
    "#### Data Collection Process:\n",
    "- Retrieved weather data using a custom Python script (`src\\getweatherdata.py`)\n",
    "- Imported CSV files into SQL Server, joined them by region, and exported a combined dataset\n",
    "\n",
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load raw avocado dataset and inspect its structure including data types and missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Define the path to the dataset file\n",
    "# Make sure this path is correct relative to where you are running your script/notebook\n",
    "file_path = '../data/raw/Avocado_HassAvocadoBoard_20152023v1.0.1.csv'\n",
    "\n",
    "# Attempt to load the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "# Print a success message and the first few rows to confirm loading\n",
    "print(\"Dataset loaded successfully!\")\n",
    "df.info()\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset contains 53,415 rows with 12 columns. All columns have complete data except for bag-related features (SmallBags, LargeBags, XLargeBags) which have about 23% missing values. Data includes date, price, volume, and categorical information about avocado types and regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract basic information about the dataset structure including dimensions and column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistical description\n",
    "print(\"Statistical Summary:\")\n",
    "df.describe()\n",
    "\n",
    "# 1. Check the dimensions (number of rows and columns)\n",
    "print(\"1. DataFrame Shape (rows, columns):\")\n",
    "print(df.shape)\n",
    "print(\"-\" * 30) # Separator\n",
    "\n",
    "\n",
    "# 4. List the column names\n",
    "print(\"\\n4. Column Names:\")\n",
    "print(df.columns)\n",
    "# More readable format:\n",
    "# print(list(df.columns))\n",
    "print(\"-\" * 30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze summary statistics for numerical and categorical variables to understand distribution, ranges, and potential data quality issues\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Step 4: Describe Data (Summary Statistics) ---\n",
    "print(\"--- Step 4: Describe Data (Summary Statistics) ---\")\n",
    "\n",
    "# 1. Summary statistics for NUMERICAL columns\n",
    "print(\"\\n1. Numerical Data Summary:\")\n",
    "# .T transposes the output for potentially better readability\n",
    "print(df.describe().T)\n",
    "print(\"-\" * 50) # Separator\n",
    "\n",
    "# 2. Summary statistics for CATEGORICAL (object/category type) columns\n",
    "print(\"\\n2. Categorical Data Summary:\")\n",
    "\n",
    "df.describe(include=['object', 'category'])\n",
    "\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Check unique values in categorical columns\n",
    "print(\"\\nUnique values in 'type':\", df['type'].unique())\n",
    "print(\"Number of unique regions:\", df['region'].nunique())\n",
    "print(\"Date range:\", df['Date'].min(), \"to\", df['Date'].max())\n",
    "\n",
    "print(\"\\nStep 4: Describe Data Complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numerical analysis reveals important insights: Average avocado prices range from $0.44 to $3.44 with a median of $1.40. Volume metrics show significant variability with wide ranges. Notably, for LargeBags and XLargeBags, the median values are 0, indicating that these packaging types are less common. The dataset spans over 8 years (2015-2023) and covers 60 distinct regions, with two product types (conventional and organic). This time range provides sufficient historical data for trend analysis and forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Quality assessments in detail\n",
    "Perform comprehensive data quality checks to identify missing values, outliers, inconsistencies, and duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Step 6: Verify Data Quality ---\")\n",
    "\n",
    "# 1. Detailed Missing Value Analysis\n",
    "print(\"\\n1. Missing Value Analysis:\")\n",
    "missing_counts = df.isnull().sum()\n",
    "missing_percent = (df.isnull().sum() / len(df)) * 100\n",
    "missing_summary = pd.DataFrame({'Missing Count': missing_counts, 'Missing Percentage': missing_percent})\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "# Filter to show only columns with missing values\n",
    "missing_summary = missing_summary[missing_summary['Missing Count'] > 0]\n",
    "\n",
    "if missing_summary.empty:\n",
    "    print(\"No missing values found in the dataset.\")\n",
    "else:\n",
    "    print(\"Columns with Missing Values:\")\n",
    "    print(missing_summary.sort_values(by='Missing Percentage', ascending=False))\n",
    "    \n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 2. Outlier Detection (Example using IQR method for AveragePrice)\n",
    "print(\"\\n2. Outlier Detection (Example: AveragePrice):\")\n",
    "\n",
    "# Using Box Plot (visual inspection - often done in Step 5)\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.boxplot(x=df['AveragePrice'])\n",
    "plt.title('Box Plot of AveragePrice for Outlier Detection')\n",
    "plt.show()\n",
    "\n",
    "# Using IQR (Interquartile Range) method\n",
    "Q1 = df['AveragePrice'].quantile(0.25)\n",
    "Q3 = df['AveragePrice'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers_iqr = df[(df['AveragePrice'] < lower_bound) | (df['AveragePrice'] > upper_bound)]\n",
    "print(f\"Number of potential outliers in AveragePrice (using IQR): {len(outliers_iqr)}\")\n",
    "print(f\"IQR bounds: Lower={lower_bound:.2f}, Upper={upper_bound:.2f}\")\n",
    "if not outliers_iqr.empty:\n",
    "    print(\"Sample 'outlier' rows based on AveragePrice IQR:\")\n",
    "    print(outliers_iqr[['Date', 'region', 'type', 'AveragePrice']].head())\n",
    "\n",
    "# Note: Repeat for other key numerical columns like TotalVolume, individual PLUs/Bags if needed.\n",
    "# Be cautious: Domain knowledge is crucial to determine if an 'outlier' is an error or a valid extreme value.\n",
    "\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 3. Inconsistency Checks\n",
    "print(\"\\n3. Inconsistency Checks:\")\n",
    "\n",
    "# a) Check value ranges/consistency for categorical data\n",
    "print(\"Unique values in 'type':\", df['type'].unique())\n",
    "# Check if only 'conventional' and 'organic' exist\n",
    "expected_types = ['conventional', 'organic']\n",
    "unexpected_types = df[~df['type'].isin(expected_types)]\n",
    "if not unexpected_types.empty:\n",
    "    print(f\"Warning: Found unexpected values in 'type' column: {unexpected_types['type'].unique()}\")\n",
    "else:\n",
    "    print(\"'type' column contains expected values.\")\n",
    "\n",
    "# b) Check date range consistency\n",
    "min_date = df['Date'].min()\n",
    "max_date = df['Date'].max()\n",
    "print(f\"Date range: {min_date.strftime('%Y-%m-%d')} to {max_date.strftime('%Y-%m-%d')}\")\n",
    "# Check if dates seem reasonable based on dataset description (2015-2023)\n",
    "\n",
    "# c) Check if component volumes add up to TotalVolume\n",
    "# Use np.isclose for floating point comparisons\n",
    "df['VolumeCheck'] = df['plu4046'] + df['plu4225'] + df['plu4770'] + df['TotalBags']\n",
    "volume_mismatch = df[~np.isclose(df['VolumeCheck'], df['TotalVolume'])]\n",
    "if not volume_mismatch.empty:\n",
    "    print(f\"\\nWarning: Found {len(volume_mismatch)} rows where PLUs + TotalBags != TotalVolume.\")\n",
    "else:\n",
    "    print(\"Sum of component volumes (PLUs + TotalBags) consistently matches TotalVolume.\")\n",
    "\n",
    "# d) Check if bag sizes add up to TotalBags\n",
    "df['BagsCheck'] = df['SmallBags'] + df['LargeBags'] + df['XLargeBags']\n",
    "bags_mismatch = df[~np.isclose(df['BagsCheck'], df['TotalBags'])]\n",
    "if not bags_mismatch.empty:\n",
    "    print(f\"\\nWarning: Found {len(bags_mismatch)} rows where Small+Large+XLarge Bags != TotalBags.\")\n",
    "else:\n",
    "    print(\"Sum of bag sizes (Small+Large+XLarge) consistently matches TotalBags.\")\n",
    "\n",
    "# Drop temporary check columns\n",
    "df.drop(columns=['VolumeCheck', 'BagsCheck'], inplace=True, errors='ignore')\n",
    "\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 4. Duplicate Records Check (Revisiting from initial checks)\n",
    "print(\"\\n4. Duplicate Records Check:\")\n",
    "duplicate_rows = df.duplicated().sum()\n",
    "print(f\"Total number of exact duplicate rows found: {duplicate_rows}\")\n",
    "\n",
    "if duplicate_rows > 0:\n",
    "    print(\"Sample duplicate rows (showing all occurrences):\")\n",
    "    # keep=False shows all rows that are part of any duplication\n",
    "    print(df[df.duplicated(keep=False)].sort_values(by=list(df.columns)).head(10)) # Sort to group duplicates\n",
    "\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"\\nStep 6: Verify Data Quality Complete.\")\n",
    "print(\"Next steps typically involve Data Cleaning based on these findings.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quality assessment reveals several issues requiring attention:\n",
    "1- Missing values: Three bag-related columns (SmallBags, LargeBags, XLargeBags) each have 23.2% missing values, suggesting systematic data collection issues.\n",
    "2- Outliers: 358 potential price outliers identified, primarily organic avocados in premium markets like San Francisco.\n",
    "3- Data inconsistencies: Serious validation issues where component volumes don't add up - 29,138 rows where PLU codes + TotalBags ≠ TotalVolume, and 35,593 rows where bag subtypes don't sum to TotalBags.\n",
    "4- No duplicates found.\n",
    "\n",
    "Note: Upon further inspection, I discovered that the missing values in bag-related columns (SmallBags, LargeBags, XLargeBags) occur systematically in records after 2022. This suggests a change in data collection methodology rather than random missing values, which will inform our imputation strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create exploratory data visualizations to understand patterns, relationships, and distributions in the avocado price dataset. These visuals will help identify key trends and inform feature selection for our predictive models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Set style for better visualization\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"Set2\")\n",
    "\n",
    "print(\"\\nLoading avocado dataset...\")\n",
    "# Load the avocado dataset\n",
    "avocado_df = pd.read_csv('../data/raw/Avocado_HassAvocadoBoard_20152023v1.0.1.csv')\n",
    "print(\"\\nAvocado dataset is loaded\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load weather data\n",
    "print(\"\\nLoading weather dataset...\")\n",
    "weather_df = pd.read_csv('../data/external/weather_data.csv')\n",
    "print(\"\\nWeather dataset is loaded\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Price Trends by Type (Conventional vs Organic)\n",
    "plt.figure(figsize=(12, 6))\n",
    "type_price_df = avocado_df.groupby(['Date', 'type'])['AveragePrice'].mean().unstack()\n",
    "type_price_df.plot(figsize=(12, 6))\n",
    "plt.title('Average Avocado Price Trends by Type (2015-2023)', fontsize=15)\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Average Price ($)', fontsize=12)\n",
    "plt.legend(title='Type')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This plot displays the average price trends for conventional and organic avocados over the entire dataset period (2015-2023). The time series reveals that organic avocados consistently command a significant price premium compared to conventional ones, typically 50-100% higher. Both types show seasonal patterns and similar directional movements, suggesting shared market drivers despite their price difference. Notable price volatility occurred around 2017-2018, possibly reflecting supply challenges or market disruptions that affected both categories. This price differential and the parallel trend patterns will be important considerations for our forecasting models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Regional Price Comparison (Top 10 regions by volume)\n",
    "if 'Total Volume' in avocado_df.columns:\n",
    "    volume_col = 'Total Volume'\n",
    "elif 'TotalVolume' in avocado_df.columns:\n",
    "    volume_col = 'TotalVolume'\n",
    "else:\n",
    "    # Try to find volume column with a different name\n",
    "    volume_cols = [col for col in avocado_df.columns if 'volume' in col.lower()]\n",
    "    if volume_cols:\n",
    "        volume_col = volume_cols[0]\n",
    "    else:\n",
    "        print(\"No volume column found, using 'TotalBags' as substitute\")\n",
    "        volume_col = 'TotalBags'\n",
    "        \n",
    "top_regions = avocado_df.groupby('region')[volume_col].sum().nlargest(10).index\n",
    "region_data = avocado_df[avocado_df['region'].isin(top_regions)]\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.boxplot(x='region', y='AveragePrice', data=region_data)\n",
    "plt.title('Average Price Distribution by Top 10 Regions', fontsize=15)\n",
    "plt.xlabel('Region', fontsize=12)\n",
    "plt.ylabel('Average Price ($)', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This boxplot compares avocado price distributions across the 10 highest-volume regions in the dataset. The visualization reveals significant regional price variations, with certain markets showing consistently higher median prices and wider price ranges than others. Major metropolitan areas typically show higher and more variable pricing, while regions closer to production centers often display more stable price patterns. These substantial regional differences highlight the importance of including location as a predictor in our forecasting models, as pricing dynamics vary by market even among the highest-volume regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Volume Distribution by Type\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='type', y=volume_col, data=avocado_df)\n",
    "plt.title('Total Volume Distribution by Type', fontsize=15)\n",
    "plt.xlabel('Type', fontsize=12)\n",
    "plt.ylabel('Total Volume', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bar chart reveals that conventional avocados vastly outsell organic varieties by volume. Despite organic avocados' price premium seen earlier, they represent a much smaller segment of the market. This significant volume difference suggests distinct market dynamics between the two types that our forecasting models should account for separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First convert the Date column to datetime format\n",
    "avocado_df['Date'] = pd.to_datetime(avocado_df['Date'])\n",
    "\n",
    "# Now create month and year columns\n",
    "avocado_df['month'] = avocado_df['Date'].dt.month\n",
    "avocado_df['year'] = avocado_df['Date'].dt.year\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "monthly_prices = avocado_df.groupby(['month', 'type'])['AveragePrice'].mean().unstack()\n",
    "monthly_prices.plot(figsize=(12, 6))\n",
    "plt.title('Seasonal Patterns in Avocado Prices', fontsize=15)\n",
    "plt.xlabel('Month', fontsize=12)\n",
    "plt.ylabel('Average Price ($)', fontsize=12)\n",
    "plt.xticks(range(1, 13), ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
    "plt.legend(title='Type')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chart reveals consistent seasonal price patterns for both conventional and organic avocados. Despite the price difference, both types show parallel behavior with summer peaks and winter lows. This shared seasonality indicates common underlying market factors affecting both varieties, which will be important seasonal features in our forecasting models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Year-over-Year Price Trends\n",
    "yearly_prices = avocado_df.groupby(['year', 'type'])['AveragePrice'].mean().unstack()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "yearly_prices.plot(figsize=(12, 6), marker='o')\n",
    "plt.title('Year-over-Year Average Price Trends (2015-2023)', fontsize=15)\n",
    "plt.xlabel('Year', fontsize=12)\n",
    "plt.ylabel('Average Price ($)', fontsize=12)\n",
    "plt.legend(title='Type')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Price analysis shows both stability and volatility. While year-over-year prices remain relatively stable overall, we observe specific jumps during 2021-2022 likely related to COVID-19 impacts. According to research [from Inspira Farms](https://www.inspirafarms.com/avocado-market-trends-hitting-2020/), the 2016-2020 price increases were driven by growing consumer demand as avocados gained popularity. When accounting for inflation, the real price increases have been modest. These observations suggest our models should incorporate external factors like pandemic effects and consumer preference shifts alongside traditional market dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plu_sales = avocado_df[['plu4046', 'plu4225', 'plu4770']].sum()\n",
    "plu_sales.plot(kind='pie', autopct='%1.1f%%', figsize=(5, 5))\n",
    "plt.title('Sales Distribution by PLU Code')\n",
    "plt.ylabel('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pie chart displays the sales distribution across different avocado sizes based on PLU (Price Look-Up) codes. The visualization reveals a clear consumer preference hierarchy: small Hass avocados (PLU 4046) are most popular, followed by large Hass avocados (PLU 4225), with medium-sized Hass avocados (PLU 4770) representing the smallest market share. This size preference information provides valuable insight for inventory management and pricing strategies, as it indicates which product sizes might be most sensitive to price changes in terms of consumer demand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average Price of Avocados by Month Across Years\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Create month and year columns if they don't exist\n",
    "if 'month' not in avocado_df.columns:\n",
    "    avocado_df['month'] = avocado_df['Date'].dt.month\n",
    "if 'year' not in avocado_df.columns:\n",
    "    avocado_df['year'] = avocado_df['Date'].dt.year\n",
    "\n",
    "# Group by month and year to get average price\n",
    "monthly_yearly_prices = avocado_df.groupby(['year', 'month'])['AveragePrice'].mean().reset_index()\n",
    "\n",
    "# Create a pivot table for better visualization\n",
    "price_pivot = monthly_yearly_prices.pivot(index='month', columns='year', values='AveragePrice')\n",
    "\n",
    "# Plot heatmap\n",
    "sns.heatmap(price_pivot, cmap='YlGnBu', annot=True, fmt='.2f', linewidths=.5)\n",
    "plt.title('Average Avocado Price by Month Across Years (2015-2023)', fontsize=16)\n",
    "plt.xlabel('Year', fontsize=12)\n",
    "plt.ylabel('Month', fontsize=12)\n",
    "\n",
    "# Set the month labels\n",
    "month_labels = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "plt.yticks(np.arange(0.5, 12.5), month_labels, rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Alternative: Line plot showing monthly trends for each year\n",
    "plt.figure(figsize=(14, 8))\n",
    "for year in sorted(avocado_df['year'].unique()):\n",
    "    year_data = monthly_yearly_prices[monthly_yearly_prices['year'] == year]\n",
    "    plt.plot(year_data['month'], year_data['AveragePrice'], marker='o', label=str(year))\n",
    "\n",
    "plt.title('Average Avocado Price by Month for Each Year (2015-2023)', fontsize=16)\n",
    "plt.xlabel('Month', fontsize=12)\n",
    "plt.ylabel('Average Price ($)', fontsize=12)\n",
    "plt.xticks(range(1, 13), month_labels)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(title='Year', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heatmap and line plot reveal clear seasonal patterns in avocado pricing across years (2015-2023). Prices typically begin rising in July, peak during summer months (July-September), and then decline through December. This pattern likely reflects the intersection of higher summer demand for gatherings and lower domestic production during these months. Conversely, prices consistently reach their lowest points in winter and early spring (January-March), coinciding with abundant imports from Mexico. \n",
    "\n",
    "The visualizations also highlight year-specific anomalies, particularly the unique price pattern in 2020 that diverges from typical seasonality, likely reflecting COVID-19's impact on supply chains and consumption patterns. These consistent seasonal cycles, along with identifiable disruptions during external events, provide strong temporal features for our forecasting models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corelation\n",
    "Correlation analysis helps identify relationships between variables that influence avocado prices. By measuring how strongly features like volume, region, season, and weather correlate with price, we can:\n",
    "\n",
    "1. Discover key price drivers (e.g., whether total volume strongly affects price)\n",
    "2. Identify redundant features to avoid multicollinearity in our models\n",
    "3. Quantify the strength of relationships we observed visually in earlier plots\n",
    "4. Inform feature selection for our predictive models by prioritizing variables with stronger price relationships\n",
    "\n",
    "Understanding these correlations will help build more accurate forecasting models by focusing on the most influential factors affecting avocado prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Correlation Heatmap\n",
    "print(\"\\n5. Plotting correlation heatmap...\")\n",
    "# Select only numerical columns for correlation\n",
    "numerical_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "# Optional: remove columns that are sums of others if desired (e.g., TotalVolume, TotalBags)\n",
    "# cols_for_corr = [col for col in numerical_cols if col not in ['TotalVolume', 'TotalBags', 'year', 'Week']] # Example exclusion\n",
    "cols_for_corr = numerical_cols # Or keep all numerical\n",
    "\n",
    "correlation_matrix = df[cols_for_corr].corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\") # Set annot=True to display labels\n",
    "plt.title('Correlation Matrix of Numerical Features')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nStep 5: Explore Data (Visualization) Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualization showing how strongly numerical variables in the avocado dataset relate to each other. It calculates correlation coefficients between all pairs of numeric features and displays them in a color-coded heatmap with exact values. This helps identify which factors most influence avocado prices and which variables might be redundant in our modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation and Common Strategies for Missing Values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The missing values analysis examines data gaps in our avocado dataset that could affect model quality. We'll identify which variables have missing data, quantify the extent of missingness, determine if patterns exist (like the post-2022 bag data), and develop appropriate handling strategies (imputation or removal). Proper treatment of missing values is essential for building reliable price forecasting models, as incomplete data can lead to biased predictions or reduced model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your DataFrame\n",
    "missing_mask = df['SmallBags'].isnull() # Mask for rows where SmallBags is missing (same for Large/XLarge)\n",
    "\n",
    "# Check TotalBags in rows where individual bags are missing\n",
    "print(\"Analysis of rows where bag sizes are missing:\")\n",
    "print(f\"Number of rows with missing bag sizes: {missing_mask.sum()}\")\n",
    "\n",
    "if missing_mask.sum() > 0:\n",
    "    # Get the TotalBags values in these specific rows\n",
    "    total_bags_in_missing_rows = df.loc[missing_mask, 'TotalBags']\n",
    "\n",
    "    # Check if TotalBags itself is missing in these rows\n",
    "    print(f\"\\nAre 'TotalBags' also missing in these rows? {total_bags_in_missing_rows.isnull().any()}\")\n",
    "    print(f\"Number of missing 'TotalBags' in these rows: {total_bags_in_missing_rows.isnull().sum()}\")\n",
    "\n",
    "    # --- Check if non-missing bags sum up to TotalBags ---\n",
    "    # Fill NaNs temporarily with 0 ONLY for this check\n",
    "    temp_df = df[['SmallBags', 'LargeBags', 'XLargeBags', 'TotalBags']].copy()\n",
    "    temp_df.fillna(0, inplace=True)\n",
    "\n",
    "    # Calculate sum where original data was missing\n",
    "    calculated_sum_where_missing = temp_df.loc[missing_mask, ['SmallBags', 'LargeBags', 'XLargeBags']].sum(axis=1)\n",
    "    total_bags_where_missing = temp_df.loc[missing_mask, 'TotalBags']\n",
    "\n",
    "    # See if the sum of bags (with NaNs treated as 0) equals TotalBags in those rows\n",
    "    mismatch_check = ~np.isclose(calculated_sum_where_missing, total_bags_where_missing)\n",
    "\n",
    "    if mismatch_check.sum() == 0:\n",
    "        print(\"\\nConclusion: In all rows with missing individual bag sizes, TotalBags seems consistent with the sum if missing values are treated as 0.\")\n",
    "        print(\"=> Recommendation: Impute missing SmallBags, LargeBags, XLargeBags with 0.\")\n",
    "        imputation_strategy = 0 # Set recommended strategy\n",
    "    else:\n",
    "        print(f\"\\nWarning: Found {mismatch_check.sum()} rows where treating missing bags as 0 does NOT align with TotalBags.\")\n",
    "        print(\"=> Consider Median imputation or investigate these specific rows further.\")\n",
    "        imputation_strategy = 'median' # Set alternative strategy\n",
    "else:\n",
    "    print(\"\\nNo missing bag sizes found for this check.\")\n",
    "    imputation_strategy = None # No action needed\n",
    "\n",
    "print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This analysis examines the 12,390 rows with missing bag size data to determine the best imputation approach. Notably, TotalBags values are present in all these rows, but simply using zeros for missing bag sizes would not align with the reported TotalBags in 12,389 cases. This indicates that these missing values represent actual bag sales rather than zeros. Therefore, I'll implement a proportional imputation strategy based on average distributions from non-missing data to ensure the individual bag sizes sum correctly to the reported TotalBags value in each row, maintaining data consistency while providing realistic estimates for the missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code performs proportional imputation for missing bag size data, distributing TotalBags values across SmallBags, LargeBags, and XLargeBags based on historical distribution patterns while ensuring mathematical consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to impute\n",
    "cols_to_impute = ['SmallBags', 'LargeBags', 'XLargeBags']\n",
    "missing_mask = df['SmallBags'].isnull()\n",
    "\n",
    "print(\"Analysis of rows where bag sizes are missing:\")\n",
    "print(f\"Number of rows with missing bag sizes: {missing_mask.sum()}\")\n",
    "\n",
    "if missing_mask.sum() > 0:\n",
    "    # Get the TotalBags values in these specific rows\n",
    "    total_bags_in_missing_rows = df.loc[missing_mask, 'TotalBags']\n",
    "\n",
    "    # Check if TotalBags itself is missing in these rows\n",
    "    print(f\"\\nAre 'TotalBags' also missing in these rows? {total_bags_in_missing_rows.isnull().any()}\")\n",
    "    print(f\"Number of missing 'TotalBags' in these rows: {total_bags_in_missing_rows.isnull().sum()}\")\n",
    "    \n",
    "    # Intelligent imputation approach based on TotalBags\n",
    "    print(\"\\nPerforming intelligent imputation using TotalBags values...\")\n",
    "    \n",
    "    # Get average proportions of bag sizes from rows with complete data\n",
    "    complete_data = df.dropna(subset=['SmallBags', 'LargeBags', 'XLargeBags'])\n",
    "    \n",
    "    if len(complete_data) > 0:\n",
    "        # Calculate average proportions where TotalBags > 0\n",
    "        total_with_bags = complete_data[complete_data['TotalBags'] > 0]\n",
    "        avg_small_prop = (total_with_bags['SmallBags'] / total_with_bags['TotalBags']).mean()\n",
    "        avg_large_prop = (total_with_bags['LargeBags'] / total_with_bags['TotalBags']).mean()\n",
    "        avg_xlarge_prop = (total_with_bags['XLargeBags'] / total_with_bags['TotalBags']).mean()\n",
    "        \n",
    "        # Display the proportions\n",
    "        print(f\"Average proportions from complete data:\")\n",
    "        print(f\"  - SmallBags: {avg_small_prop:.4f}\")\n",
    "        print(f\"  - LargeBags: {avg_large_prop:.4f}\")\n",
    "        print(f\"  - XLargeBags: {avg_xlarge_prop:.4f}\")\n",
    "        \n",
    "        # Apply these proportions to distribute TotalBags across the missing values\n",
    "        for idx in df[missing_mask].index:\n",
    "            total_bags = df.loc[idx, 'TotalBags']\n",
    "            \n",
    "            # Distribute total bags according to the average proportions\n",
    "            df.loc[idx, 'SmallBags'] = total_bags * avg_small_prop\n",
    "            df.loc[idx, 'LargeBags'] = total_bags * avg_large_prop\n",
    "            df.loc[idx, 'XLargeBags'] = total_bags * avg_xlarge_prop\n",
    "        \n",
    "        # Verify the sum equals TotalBags (within rounding error)\n",
    "        imputed_rows = df.loc[missing_mask]\n",
    "        sum_of_bags = imputed_rows['SmallBags'] + imputed_rows['LargeBags'] + imputed_rows['XLargeBags']\n",
    "        total_bags_values = imputed_rows['TotalBags']\n",
    "        \n",
    "        # Check if sums match within tolerance\n",
    "        is_close = np.isclose(sum_of_bags, total_bags_values, rtol=1e-5)\n",
    "        pct_close = (is_close.sum() / len(is_close)) * 100\n",
    "        \n",
    "        print(f\"\\nVerification: {pct_close:.2f}% of imputed rows have bag sums matching TotalBags within tolerance\")\n",
    "        \n",
    "        if pct_close < 100:\n",
    "            print(\"Adjusting remaining rows to ensure exact match with TotalBags...\")\n",
    "            \n",
    "            # For any rows where the sum doesn't match TotalBags exactly, adjust SmallBags to make it match\n",
    "            for idx in imputed_rows.index:\n",
    "                if not np.isclose(\n",
    "                    df.loc[idx, 'SmallBags'] + df.loc[idx, 'LargeBags'] + df.loc[idx, 'XLargeBags'],\n",
    "                    df.loc[idx, 'TotalBags'],\n",
    "                    rtol=1e-5\n",
    "                ):\n",
    "                    current_sum = df.loc[idx, 'SmallBags'] + df.loc[idx, 'LargeBags'] + df.loc[idx, 'XLargeBags']\n",
    "                    adjustment = df.loc[idx, 'TotalBags'] - current_sum\n",
    "                    df.loc[idx, 'SmallBags'] += adjustment  # Adjust SmallBags to ensure exact sum\n",
    "    else:\n",
    "        print(\"No complete rows found to calculate proportions. Using equal distribution.\")\n",
    "        \n",
    "        # Divide TotalBags equally among the three bag types\n",
    "        for idx in df[missing_mask].index:\n",
    "            total_bags = df.loc[idx, 'TotalBags']\n",
    "            df.loc[idx, 'SmallBags'] = total_bags / 3\n",
    "            df.loc[idx, 'LargeBags'] = total_bags / 3\n",
    "            df.loc[idx, 'XLargeBags'] = total_bags / 3\n",
    "        \n",
    "    print(\"Imputation strategy: Proportional distribution based on TotalBags\")\n",
    "else:\n",
    "    print(\"\\nNo missing bag sizes found for this check.\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Final verification\n",
    "print(\"\\nMissing values AFTER imputation:\")\n",
    "print(df[cols_to_impute].isnull().sum())\n",
    "\n",
    "# Check that imputed values sum to TotalBags\n",
    "sum_of_bags = df['SmallBags'] + df['LargeBags'] + df['XLargeBags']\n",
    "diff = np.abs(sum_of_bags - df['TotalBags'])\n",
    "max_diff = diff.max()\n",
    "print(f\"\\nMaximum difference between sum of bags and TotalBags: {max_diff:.10f}\")\n",
    "print(f\"Mean difference: {diff.mean():.10f}\")\n",
    "\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Detection and Analysis\n",
    "\n",
    "This analysis identifies potential outliers in key variables using the IQR method, visualizing their distributions and quantifying their extent for informed treatment decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Assuming df is loaded, missing values & duplicates handled ---\n",
    "\n",
    "print(\"--- Data Preparation: Investigating Outliers ---\")\n",
    "\n",
    "# --- AveragePrice ---\n",
    "print(\"\\nInvestigating Outliers in 'AveragePrice':\")\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df['AveragePrice'], kde=True)\n",
    "plt.title('Histogram of AveragePrice')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(x=df['AveragePrice'])\n",
    "plt.title('Box Plot of AveragePrice')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate IQR bounds for AveragePrice\n",
    "Q1_price = df['AveragePrice'].quantile(0.25)\n",
    "Q3_price = df['AveragePrice'].quantile(0.75)\n",
    "IQR_price = Q3_price - Q1_price\n",
    "lower_bound_price = Q1_price - 1.5 * IQR_price\n",
    "upper_bound_price = Q3_price + 1.5 * IQR_price\n",
    "\n",
    "outliers_price = df[(df['AveragePrice'] < lower_bound_price) | (df['AveragePrice'] > upper_bound_price)]\n",
    "print(f\"Potential outliers in AveragePrice using IQR: {len(outliers_price)}\")\n",
    "print(f\"  - Lower Bound: {lower_bound_price:.2f}\")\n",
    "print(f\"  - Upper Bound: {upper_bound_price:.2f}\")\n",
    "if not outliers_price.empty:\n",
    "    print(\"  - Min/Max of potential outliers:\", outliers_price['AveragePrice'].min(), \"-\", outliers_price['AveragePrice'].max())\n",
    "    # print(\"  - Sample outliers:\\n\", outliers_price[['Date', 'region', 'type', 'AveragePrice']].head())\n",
    "\n",
    "\n",
    "# --- TotalVolume ---\n",
    "# Often skewed, log transform helps visualize and assess outliers\n",
    "print(\"\\nInvestigating Outliers in 'TotalVolume' (using log scale):\")\n",
    "df['LogTotalVolume'] = np.log1p(df['TotalVolume']) # log1p = log(1+x)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df['LogTotalVolume'], kde=True)\n",
    "plt.title('Histogram of log(TotalVolume + 1)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(x=df['LogTotalVolume'])\n",
    "plt.title('Box Plot of log(TotalVolume + 1)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate IQR bounds for LogTotalVolume\n",
    "Q1_vol = df['LogTotalVolume'].quantile(0.25)\n",
    "Q3_vol = df['LogTotalVolume'].quantile(0.75)\n",
    "IQR_vol = Q3_vol - Q1_vol\n",
    "lower_bound_vol = Q1_vol - 1.5 * IQR_vol\n",
    "upper_bound_vol = Q3_vol + 1.5 * IQR_vol\n",
    "\n",
    "# Find outliers based on the log scale\n",
    "outliers_log_vol_indices = df[(df['LogTotalVolume'] < lower_bound_vol) | (df['LogTotalVolume'] > upper_bound_vol)].index\n",
    "outliers_vol = df.loc[outliers_log_vol_indices] # Get original scale rows\n",
    "\n",
    "print(f\"Potential outliers in LogTotalVolume using IQR: {len(outliers_vol)}\")\n",
    "print(f\"  - Lower Bound (log): {lower_bound_vol:.2f}\")\n",
    "print(f\"  - Upper Bound (log): {upper_bound_vol:.2f}\")\n",
    "if not outliers_vol.empty:\n",
    "     print(\"  - Min/Max TotalVolume of potential outliers:\", outliers_vol['TotalVolume'].min(), \"-\", outliers_vol['TotalVolume'].max())\n",
    "     # print(\"  - Sample outliers:\\n\", outliers_vol[['Date', 'region', 'type', 'TotalVolume']].head())\n",
    "\n",
    "# Clean up temporary column\n",
    "df.drop(columns=['LogTotalVolume'], inplace=True, errors='ignore')\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"Outlier investigation complete. Decision needed on handling.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outlier analysis reveals two distinct patterns: 358 price outliers (2.55-3.44) exceeding the upper bound of $2.55, primarily representing premium organic avocados in high-cost markets; and a single volume outlier with an unusually low value (84.56), potentially indicating a data entry error or special circumstance. These findings suggest different treatment approaches may be needed - the price outliers likely represent valid premium market segments and could be retained, while the single volume outlier warrants further investigation or removal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing outliers\n",
    "Outliers can skew statistical analyses and machine learning models, distorting measures like the mean and increasing error variance, leading to inaccurate conclusions and reduced model accuracy. However, removing them shouldn't be automatic; it's crucial to understand their cause and context. Sometimes they represent genuine extreme values, and alternative approaches like robust statistics or data transformations might be more suitable than outright removal, ensuring that valuable information isn't lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"--- Data Preparation: Handling Outliers ---\")\n",
    "\n",
    "# --- Handle TotalVolume Outlier ---\n",
    "# Find the index of the outlier row identified previously\n",
    "try:\n",
    "    log_total_volume = np.log1p(df['TotalVolume'])\n",
    "    Q1_vol = log_total_volume.quantile(0.25)\n",
    "    Q3_vol = log_total_volume.quantile(0.75)\n",
    "    IQR_vol = Q3_vol - Q1_vol\n",
    "    lower_bound_vol = Q1_vol - 1.5 * IQR_vol\n",
    "    upper_bound_vol = Q3_vol + 1.5 * IQR_vol\n",
    "    outlier_vol_indices = df[(log_total_volume < lower_bound_vol) | (log_total_volume > upper_bound_vol)].index\n",
    "\n",
    "    if not outlier_vol_indices.empty:\n",
    "        print(f\"\\nRemoving {len(outlier_vol_indices)} outlier row(s) based on LogTotalVolume IQR.\")\n",
    "        print(f\"Index(es) to remove: {outlier_vol_indices.tolist()}\")\n",
    "        # Optional: print the row(s) before removing\n",
    "        # print(\"Outlier row(s) being removed:\")\n",
    "        # print(df.loc[outlier_vol_indices])\n",
    "        df.drop(outlier_vol_indices, inplace=True)\n",
    "        print(f\"Removed {len(outlier_vol_indices)} row(s). New shape: {df.shape}\")\n",
    "    else:\n",
    "        print(\"\\nNo outliers found for TotalVolume based on Log IQR to remove.\")\n",
    "\n",
    "except KeyError:\n",
    "    print(\"\\n'TotalVolume' column not found, skipping volume outlier removal.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred during volume outlier removal: {e}\")\n",
    "\n",
    "\n",
    "try:\n",
    "    price_cap_value = df['AveragePrice'].quantile(0.99) # Calculate 99th percentile\n",
    "    print(f\"\\nCapping AveragePrice outliers above the 99th percentile ({price_cap_value:.2f}).\")\n",
    "\n",
    "    # Identify values above the cap\n",
    "    price_outliers_mask = df['AveragePrice'] > price_cap_value\n",
    "    num_price_outliers_to_cap = price_outliers_mask.sum()\n",
    "\n",
    "    if num_price_outliers_to_cap > 0:\n",
    "        # Replace values above the cap with the cap value\n",
    "        df.loc[price_outliers_mask, 'AveragePrice'] = price_cap_value\n",
    "        print(f\"Capped {num_price_outliers_to_cap} values in 'AveragePrice'.\")\n",
    "        print(f\"New maximum AveragePrice: {df['AveragePrice'].max():.2f}\")\n",
    "    else:\n",
    "        print(\"No values in 'AveragePrice' were above the 99th percentile cap.\")\n",
    "\n",
    "except KeyError:\n",
    "    print(\"\\n'AveragePrice' column not found, skipping price outlier capping.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred during price outlier capping: {e}\")\n",
    "\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"Outlier handling step complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "single outlier was removed from 'LogTotalVolume', and 535 'AveragePrice' values exceeding the 99th percentile (2.46) were capped at 2.46 to mitigate their impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation Complete: Moving to Modeling Phase\n",
    "Now that we've completed the data cleaning, imputation, and outlier analysis, we'll save this processed dataset and transition to the modeling phase where we'll develop and evaluate various price forecasting models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "df_cleaned = df.copy()\n",
    "\n",
    "\n",
    "print(\"\\nSaving cleaned data...\")\n",
    "df_cleaned.to_csv(\"../data/processed/cleaned_avocado_data.csv\", index=False)\n",
    "print(\"Cleaned data saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering for Avocado Price Prediction\n",
    "This section transforms our cleaned dataset into optimized model inputs by creating relevant temporal features, encoding categorical variables, and selecting the most predictive attributes to improve forecasting accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcleaned = pd.read_csv('../data/processed/cleaned_avocado_data.csv')\n",
    "del df\n",
    "\n",
    "print(\"--- Data Preparation: Verifying Data Types ---\")\n",
    "print(dfcleaned.info())\n",
    "print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets converts dates to datetime format, extracts time features (year, month, week, day), maps avocado types to binary values, creates dummy variables for regions, and displays the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Assuming df is loaded, cleaned, and 'Date' column exists ---\n",
    "\n",
    "print(\"--- Data Preparation: Feature Engineering (Time Features) - Corrected ---\")\n",
    "\n",
    "print(\"Converting 'Date' column to datetime type...\")\n",
    "dfcleaned['Date'] = pd.to_datetime(dfcleaned['Date'])\n",
    "    # --- Confirmed 'Date' column exists and is datetime ---\n",
    "\n",
    "    # Extract time-based features using the .dt accessor\n",
    "print(\"Extracting features using '.dt' accessor...\")\n",
    "dfcleaned['Year'] = dfcleaned['Date'].dt.year\n",
    "dfcleaned['Month'] = dfcleaned['Date'].dt.month\n",
    "dfcleaned['WeekOfYear'] = dfcleaned['Date'].dt.isocalendar().week.astype(int)\n",
    "dfcleaned['DayOfYear'] = dfcleaned['Date'].dt.dayofyear\n",
    "dfcleaned['DayOfWeek'] = dfcleaned['Date'].dt.dayofweek # Monday=0, Sunday=6\n",
    "\n",
    "print(\"\\nCreated new time-based features: Year, Month, WeekOfYear, DayOfYear, DayOfWeek\")\n",
    "print(\"Sample with new features:\")\n",
    "# map (AKA factored) the type column to 0 and 1\n",
    "dfcleaned[\"type\"] = dfcleaned[\"type\"].map({\"conventional\": 0, \"organic\": 1})\n",
    "\n",
    "# Turn the region (which is categorical) into binary columns (Dummy Variables))\n",
    "dfcleaned = pd.get_dummies(dfcleaned, columns=[\"region\"], drop_first=True)\n",
    "dfcleaned.info()\n",
    "\n",
    "# Show Date and the new columns\n",
    "#dfcleaned.info()\n",
    "print(dfcleaned.head())\n",
    "\n",
    "dfcleaned[['Date', 'Year', 'Month', 'WeekOfYear', 'DayOfYear', 'DayOfWeek']].head()\n",
    "\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"Feature engineering step (time features) complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transforms the avocado dataset for machine learning by converting dates into time features (year, month, week, day), changing avocado type to numbers (0/1), and converting regions into binary columns (one-hot encoding). This increases columns because each region becomes its own column, but it's necessary to make categorical data work with machine learning models, helping them better understand patterns and predict avocado prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Model Development\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chronological Time-Series Data Split for Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step prepares the avocado dataset for machine learning by selecting appropriate numeric features, separating the target variable (AveragePrice), and performing a time-ordered split with 80% training and 20% testing data to maintain the temporal integrity essential for accurate price forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Assuming df is loaded and ONE-HOT ENCODED ---\n",
    "\n",
    "print(\"--- Data Preparation: Defining Features and Splitting Data (REVISED - ENSURE THIS RUNS) ---\")\n",
    "\n",
    "# --- Define features (X) and target (y) more robustly ---\n",
    "target_variable = 'AveragePrice'\n",
    "\n",
    "# Select only columns with numeric data types for features AFTER encoding\n",
    "print(\"Selecting numerical features...\")\n",
    "numeric_cols = dfcleaned.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "# Ensure target variable itself is not included in features\n",
    "if target_variable in numeric_cols:\n",
    "    print(f\"Removing target '{target_variable}' from feature list.\")\n",
    "    numeric_cols.remove(target_variable)\n",
    "else:\n",
    "    print(f\"Target '{target_variable}' not found in initial numeric columns (as expected).\")\n",
    "    if target_variable not in dfcleaned.columns:\n",
    "         raise ValueError(f\"Target variable '{target_variable}' not found in DataFrame columns!\")\n",
    "\n",
    "# Check for any other known non-feature columns that might be numeric (e.g., IDs)\n",
    "if 'Unnamed: 0' in numeric_cols:\n",
    "    print(\"Removing 'Unnamed: 0' column from features.\")\n",
    "    numeric_cols.remove('Unnamed: 0')\n",
    "\n",
    "# Define features and target using the selected numeric columns\n",
    "features = dfcleaned[numeric_cols]\n",
    "target = dfcleaned[target_variable]\n",
    "\n",
    "print(f\"Selected {len(features.columns)} features for X.\")\n",
    "# print(\"Features included:\", features.columns.tolist()) # Uncomment to verify features\n",
    "# --- End of revised feature selection ---\n",
    "\n",
    "\n",
    "# 1. Ensure data is sorted by Date (CRITICAL for time-series split)\n",
    "# Make sure 'Date' column exists if you are sorting by it\n",
    "if 'Date' in dfcleaned.columns and pd.api.types.is_datetime64_any_dtype(dfcleaned['Date']):\n",
    "    print(\"Sorting DataFrame by 'Date' column...\")\n",
    "    # Create a temporary sorted df to get indices, avoids modifying original df unless needed\n",
    "    df_sorted_indices = dfcleaned.sort_values(by='Date').index\n",
    "else:\n",
    "    # If 'Date' column doesn't exist or isn't datetime, check if index is datetime and sorted\n",
    "    if isinstance(dfcleaned.index, pd.DatetimeIndex):\n",
    "         print(\"Using DataFrame index (assuming it's datetime and sorted)...\")\n",
    "         df_sorted_indices = dfcleaned.index # Assume index is already sorted\n",
    "    else:\n",
    "         raise ValueError(\"Cannot determine sorting order. 'Date' column missing/incorrect or index not DatetimeIndex.\")\n",
    "\n",
    "\n",
    "# 2. Determine the split point (Using the sorted indices length)\n",
    "split_ratio = 0.80\n",
    "split_index_loc = int(len(df_sorted_indices) * split_ratio)\n",
    "print(f\"Splitting data at index location: {split_index_loc} ({(split_ratio*100):.0f}% train / {((1-split_ratio)*100):.0f}% test)\")\n",
    "\n",
    "# 3. Perform the split using the sorted indices applied to 'features' and 'target'\n",
    "train_indices = df_sorted_indices[:split_index_loc]\n",
    "test_indices = df_sorted_indices[split_index_loc:]\n",
    "\n",
    "X_train = features.loc[train_indices]\n",
    "X_test = features.loc[test_indices]\n",
    "y_train = target.loc[train_indices]\n",
    "y_test = target.loc[test_indices]\n",
    "\n",
    "\n",
    "# 4. Verify the shapes of the splits\n",
    "print(\"\\nShapes of the datasets:\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "# (Verification of time range can be added back if needed)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"Data splitting step complete. Ready for scaling.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data split successfully creates a chronological division with 42,731 training samples (80%) and 10,683 test samples (20%), preserving the temporal structure essential for time series forecasting. Each sample contains 14 numerical features after removing the target variable 'AveragePrice'. This time-ordered split ensures the model is trained on past data and evaluated on future data, properly simulating real-world forecasting conditions where we predict upcoming prices using only historical information. The strict chronological separation prevents data leakage that would occur if future data influenced predictions of past prices, maintaining the integrity of our evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Necessary packages are imported for general use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression: Baseline Model for Avocado Price Prediction\n",
    "Linear regression establishes our baseline forecasting model by finding linear relationships between avocado features (volume, region, seasonality) and prices. While simple, this approach provides an interpretable starting point to understand key price drivers and benchmark performance before exploring more complex models. We'll evaluate how well a linear combination of our features can approximate avocado price fluctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(\"../data/processed/cleaned_avocado_data.csv\")\n",
    "\n",
    "# Basic preprocessing\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "df[\"type\"] = df[\"type\"].map({\"conventional\": 0, \"organic\": 1})\n",
    "df['month'] = df['Date'].dt.month\n",
    "df['year'] = df['Date'].dt.year\n",
    "\n",
    "# Select a smaller sample to avoid memory issues (optional, remove if not needed)\n",
    "df_sample = df.sample(frac=0.5, random_state=42)\n",
    "\n",
    "# Get only numeric columns\n",
    "numeric_cols = df_sample.select_dtypes(include=['int64', 'float64']).columns\n",
    "X = df_sample[numeric_cols].drop(['AveragePrice'], axis=1, errors='ignore')\n",
    "y = df_sample[\"AveragePrice\"]\n",
    "\n",
    "print(\"\\nFeatures being used in the model:\", X.columns.tolist())\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 1. First, train a standard linear regression model as baseline\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "y_pred_lr = lin_reg.predict(X_test)\n",
    "\n",
    "# Calculate baseline metrics\n",
    "mae_lr = mean_absolute_error(y_test, y_pred_lr)\n",
    "rmse_lr = np.sqrt(mean_squared_error(y_test, y_pred_lr))\n",
    "r2_lr = r2_score(y_test, y_pred_lr)\n",
    "\n",
    "print(\"\\nBaseline Linear Regression Metrics:\")\n",
    "print(f\"  Mean Absolute Error (MAE): {mae_lr:.4f}\")\n",
    "print(f\"  Root Mean Squared Error (RMSE): {rmse_lr:.4f}\")\n",
    "print(f\"  R-squared (R²): {r2_lr:.4f}\")\n",
    "\n",
    "# Feature importance for baseline model\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Coefficient': lin_reg.coef_\n",
    "})\n",
    "print(\"\\nBaseline Feature Importance:\")\n",
    "print(feature_importance.sort_values(by='Coefficient', key=abs, ascending=False).head(10))\n",
    "\n",
    "# 2. Now create an optimized pipeline with reduced resource requirements\n",
    "pipeline = Pipeline([\n",
    "    ('poly', PolynomialFeatures(degree=1)),  # Start with simpler degree\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "# Simplified parameter grid\n",
    "param_grid = {\n",
    "    'poly__degree': [1, 2],  # Reduced from [1, 2, 3]\n",
    "    'regressor__fit_intercept': [True]  # Reduced from [True, False]\n",
    "}\n",
    "\n",
    "# Create a more efficient GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=3,  # Reduced from 5\n",
    "    scoring='r2',\n",
    "    n_jobs=1,  # Use single thread to avoid resource issues\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit the grid search\n",
    "print(\"\\nTraining tuned model with GridSearchCV...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and score\n",
    "print(\"\\nBest parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation R² score:\", grid_search.best_score_)\n",
    "\n",
    "# Get predictions using the best model\n",
    "y_pred_tuned = grid_search.predict(X_test)\n",
    "\n",
    "# Calculate metrics for the tuned model\n",
    "mae_tuned = mean_absolute_error(y_test, y_pred_tuned)\n",
    "rmse_tuned = np.sqrt(mean_squared_error(y_test, y_pred_tuned))\n",
    "r2_tuned = r2_score(y_test, y_pred_tuned)\n",
    "\n",
    "print(\"\\nTuned Model Metrics:\")\n",
    "print(f\"  Mean Absolute Error (MAE): {mae_tuned:.4f}\")\n",
    "print(f\"  Root Mean Squared Error (RMSE): {rmse_tuned:.4f}\")\n",
    "print(f\"  R-squared (R²): {r2_tuned:.4f}\")\n",
    "\n",
    "# Compare with the original model\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(\"Baseline Model R²:\", r2_lr)\n",
    "print(\"Tuned Model R²:\", r2_tuned)\n",
    "improvement = ((r2_tuned - r2_lr) / r2_lr * 100)\n",
    "print(f\"Improvement: {improvement:.2f}%\")\n",
    "\n",
    "# Plot actual vs predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred_tuned, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Price')\n",
    "plt.ylabel('Predicted Price')\n",
    "plt.title('Actual vs Predicted Avocado Prices (Tuned Model)')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Performance Analysis\n",
    "\n",
    "This baseline model demonstrates the limitations of linear approaches for avocado price prediction. With a baseline R² of 0.453 and only marginal improvement to 0.470 after polynomial feature tuning, linear regression captures less than half of the price variation. Feature importance reveals \"type\" (organic vs. conventional) as overwhelmingly influential, while volume and bag features show minimal linear correlation with price.\n",
    "\n",
    "The model's weak performance confirms that avocado price dynamics are inherently non-linear, involving complex interactions between features and seasonal patterns that linear regression cannot adequately capture. The introduction of polynomial features (degree=2) helped slightly but couldn't overcome these fundamental limitations.\n",
    "\n",
    "These results establish an important baseline but clearly indicate the need for more sophisticated models that can handle non-linear relationships and interaction effects between seasonality, region, and volume metrics to achieve better predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Gradient Boosting Models for Avocado Price Prediction\n",
    "\n",
    "Gradient boosting techniques (XGBoost and GBR) are applied to create more advanced models that can capture the complex non-linear relationships in avocado pricing data. These models sequentially build an ensemble of weak learners to create a powerful predictive system capable of handling the seasonal patterns and market dynamics affecting prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read and prepare the data\n",
    "print(\"Loading data...\")\n",
    "dfxgboost = pd.read_csv('../data/processed/cleaned_avocado_data.csv')\n",
    "\n",
    "# Convert date to datetime\n",
    "dfxgboost['Date'] = pd.to_datetime(dfxgboost['Date'])\n",
    "\n",
    "# Create additional time-based features\n",
    "dfxgboost['Month'] = dfxgboost['Date'].dt.month\n",
    "dfxgboost['DayOfWeek'] = dfxgboost['Date'].dt.dayofweek\n",
    "dfxgboost['WeekOfYear'] = dfxgboost['Date'].dt.isocalendar().week\n",
    "dfxgboost['Year'] = dfxgboost['Date'].dt.year\n",
    "\n",
    "# Encode categorical variables\n",
    "le_region = LabelEncoder()\n",
    "le_type = LabelEncoder()\n",
    "dfxgboost['region_encoded'] = le_region.fit_transform(dfxgboost['region'])\n",
    "dfxgboost['type_encoded'] = le_type.fit_transform(dfxgboost['type'])\n",
    "\n",
    "# Feature selection\n",
    "features = [\n",
    "    'region_encoded', 'type_encoded', 'Year', 'Month', 'WeekOfYear', 'DayOfWeek',\n",
    "    'TotalVolume'\n",
    "]\n",
    "\n",
    "X = dfxgboost[features]\n",
    "y = dfxgboost['AveragePrice']\n",
    "\n",
    "# Print feature statistics\n",
    "print(\"\\nFeature Statistics:\")\n",
    "print(X.describe())\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a pipeline with imputer, scaler, and model\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', GradientBoostingRegressor(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5,\n",
    "        min_samples_split=5,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Train the pipeline\n",
    "print(\"\\nTraining model pipeline...\")\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Model evaluation\n",
    "y_pred = pipeline.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\n=== Model Performance ===\")\n",
    "print(f\"Root Mean Squared Error: ${rmse:.3f}\")\n",
    "print(f\"Mean Absolute Error: ${mae:.3f}\")\n",
    "print(f\"R-squared Score: {r2:.3f}\")\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='r2')\n",
    "print(f\"\\nCross-validation R² scores: {cv_scores}\")\n",
    "print(f\"Average CV R² score: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
    "\n",
    "# Feature importance visualization\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': features,\n",
    "    'importance': pipeline.named_steps['regressor'].feature_importances_\n",
    "})\n",
    "feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "\n",
    "def predict_price(date, region, type_val):\n",
    "    \"\"\"\n",
    "    Predict avocado price based on date, region, and type.\n",
    "    \n",
    "    Parameters:\n",
    "    date (str): Date in 'YYYY-MM-DD' format\n",
    "    region (str): Region name\n",
    "    type_val (str): 'conventional' or 'organic'\n",
    "    \n",
    "    Returns:\n",
    "    float: Predicted price\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert date to datetime and extract features\n",
    "        date = pd.to_datetime(date)\n",
    "        year = date.year\n",
    "        month = date.month\n",
    "        day_of_week = date.dayofweek\n",
    "        week_of_year = date.isocalendar().week\n",
    "        \n",
    "        # Encode categorical variables\n",
    "        region_enc = le_region.transform([region])[0]\n",
    "        type_enc = le_type.transform([type_val])[0]\n",
    "        \n",
    "        # Get average volume for the region\n",
    "        region_data = dfxgboost[dfxgboost['region'] == region]\n",
    "        if len(region_data) == 0:\n",
    "            raise ValueError(f\"Region '{region}' not found in training data\")\n",
    "            \n",
    "        avg_volume = region_data['TotalVolume'].mean()\n",
    "        \n",
    "        # Create feature vector\n",
    "        features = np.array([[\n",
    "            region_enc, type_enc, year, month, day_of_week, week_of_year,\n",
    "            avg_volume\n",
    "        ]])\n",
    "        \n",
    "        # Make prediction using the pipeline\n",
    "        predicted_price = pipeline.predict(features)[0]\n",
    "        \n",
    "        return predicted_price\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during prediction: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example prediction\n",
    "    test_date = '2023-01-15'\n",
    "    test_region = 'StLouis'\n",
    "    test_type = 'conventional'\n",
    "    \n",
    "    predicted_price = predict_price(test_date, test_region, test_type)\n",
    "    \n",
    "    \n",
    "\n",
    "    if predicted_price is not None:\n",
    "        print(f\"\\nPrediction Example:\")\n",
    "        print(f\"Date: {test_date}\")\n",
    "        print(f\"Region: {test_region}\")\n",
    "        print(f\"Type: {test_type}\")\n",
    "        print(f\"Predicted Price: ${predicted_price:.2f}\")\n",
    "        \n",
    "        # Show historical comparison\n",
    "        print(\"\\nRecent historical prices for comparison:\")\n",
    "        recent_prices = dfxgboost[\n",
    "            (dfxgboost['region'] == test_region) & \n",
    "            (dfxgboost['type'] == test_type)\n",
    "        ].sort_values('Date', ascending=False).head(5)\n",
    "        \n",
    "        for _, row in recent_prices.iterrows():\n",
    "            print(f\"Date: {row['Date'].strftime('%Y-%m-%d')}, \"\n",
    "                  f\"Price: ${row['AveragePrice']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Performance Analysis\n",
    "\n",
    "The gradient boosting models demonstrate substantially stronger predictive performance compared to linear regression, achieving an R² of 0.85-0.86 with significantly lower error metrics (MAE of $0.11-0.14). These models effectively capture the non-linear relationships between features and avocado prices that linear regression couldn't address.\n",
    "\n",
    "Feature importance analysis confirms that avocado type (organic vs. conventional) remains the dominant predictor, accounting for over 50% of predictive power. Geographic factors follow in importance, with premium markets like San Francisco, Seattle, and Hartford/Springfield showing significant influence on pricing. The strong performance across diverse regions validates the model's ability to capture regional pricing dynamics.\n",
    "\n",
    "The model demonstrates good generalization, as evidenced by consistent cross-validation scores (CV R² of 0.845 ±0.006) and accurate prediction on 2023 data (R² of 0.81). The comparison with actual recent prices for St. Louis shows reasonable predictive accuracy with some expected variance. Overall, gradient boosting provides a robust framework for avocado price forecasting that balances complexity with interpretability while significantly outperforming linear approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Model for Avocado Price Prediction\n",
    "Random Forest applies ensemble learning to create a robust model by averaging predictions from multiple decision trees, each trained on different subsets of the data. This approach excels at handling the complex interactions between regional, seasonal, and market factors affecting avocado prices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(\"../data/processed/cleaned_avocado_data.csv\")\n",
    "\n",
    "\n",
    "# Basic preprocessing\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "df[\"type\"] = df[\"type\"].map({\"conventional\": 0, \"organic\": 1})\n",
    "df['month'] = df['Date'].dt.month\n",
    "df['year'] = df['Date'].dt.year\n",
    "\n",
    "# Get only numeric columns initially\n",
    "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "print(\"\\nCategorical columns found:\", categorical_cols.tolist())\n",
    "\n",
    "# One-hot encode all categorical columns\n",
    "df = pd.get_dummies(df, columns=categorical_cols)\n",
    "\n",
    "# Prepare features and target\n",
    "# Remove Date and target variable\n",
    "X = df.drop(columns=[\"Date\", \"AveragePrice\", \"plu4046\", \"plu4225\", \"plu4770\"])\n",
    "y = df[\"AveragePrice\"]\n",
    "\n",
    "print(\"\\nFinal features:\", X.columns.tolist())\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train Random Forest model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "print(\"\\nRandom Forest R² Score:\", rf_model.score(X_test, y_test))\n",
    "# Calculate metrics\n",
    "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "rmse_rf = np.sqrt(mse_rf)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "# Show feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "})\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(feature_importance.sort_values(by='Importance', ascending=False).head(10))\n",
    "\n",
    "print(\"\\nRandom Forest Regressor Metrics:\")\n",
    "print(f\"  Mean Absolute Error (MAE): {mae_rf:.4f}\")\n",
    "print(f\"  Mean Squared Error (MSE):  {mse_rf:.4f}\")\n",
    "print(f\"  Root Mean Squared Error (RMSE): {rmse_rf:.4f}\")\n",
    "print(f\"  R-squared (R²):           {r2_rf:.4f}\")\n",
    "\n",
    "print(\"\\nLinear Regression Metrics:\")\n",
    "print(f\"  Mean Absolute Error (MAE): {mae_lr:.4f}\")\n",
    "print(f\"  Root Mean Squared Error (RMSE): {rmse_lr:.4f}\")\n",
    "print(f\"  R-squared (R²): {r2_lr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Performance Analysis\n",
    "\n",
    "The Random Forest model achieves exceptional predictive accuracy for avocado price forecasting with an R² of 0.9261, dramatically outperforming both linear regression (R² = 0.4532) and gradient boosting approaches (R² = 0.85). The model's low error metrics (MAE = 0.0743, RMSE = 0.1056) indicate highly precise predictions across different market conditions.\n",
    "\n",
    "Feature importance analysis reveals that avocado type (organic vs. conventional) dominates predictive power at 44.29%, confirming the substantial price premium for organic products. Temporal factors (month at 7.19% and year at 6.81%) and volume metrics (TotalVolume at 7.13%, TotalBags at 5.88%) also significantly influence prices, capturing both seasonal effects and supply-demand dynamics. Premium markets like San Francisco, Seattle, and Hartford/Springfield emerge as the most influential regional factors.\n",
    "\n",
    "The Random Forest model's superior performance stems from its ability to capture complex non-linear relationships and interaction effects between variables without overfitting, making it the optimal choice for avocado price forecasting where multiple factors simultaneously influence pricing dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation on 2023 Data: Real-World Prediction Performance\n",
    "Lets calculates and displays evaluation metrics (MAE, RMSE, R²) for the Random Forest model's predictions on the test dataset, comparing its performance against other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict avocado price\n",
    "def predict_avocado_price(region, date_str, avocado_type, total_volume=5000, plu4225=2000):\n",
    "    \"\"\"\n",
    "    Predict avocado price based on input parameters\n",
    "    \"\"\"\n",
    "    # Convert date to datetime\n",
    "    date = pd.to_datetime(date_str)\n",
    "    \n",
    "    # Calculate other PLUs based on typical distribution\n",
    "    plu4046 = total_volume * 0.3  # Assuming 30% are small/medium\n",
    "    plu4770 = total_volume * 0.1  # Assuming 10% are extra large\n",
    "    \n",
    "    # Create a dataframe with a single row for prediction\n",
    "    pred_data = pd.DataFrame({\n",
    "        'Date': [date],\n",
    "        'TotalVolume': [total_volume],\n",
    "        'plu4046': [plu4046],\n",
    "        'plu4225': [plu4225],\n",
    "        'plu4770': [plu4770],\n",
    "        'TotalBags': [total_volume * 0.2],\n",
    "        'SmallBags': [total_volume * 0.15],\n",
    "        'LargeBags': [total_volume * 0.04],\n",
    "        'XLargeBags': [total_volume * 0.01],\n",
    "        'type': [avocado_type],\n",
    "        'region': [region]\n",
    "    })\n",
    "    \n",
    "    # Apply the same preprocessing as in training\n",
    "    pred_data[\"type\"] = pred_data[\"type\"].map({\"conventional\": 0, \"organic\": 1})\n",
    "    pred_data['month'] = pred_data['Date'].dt.month\n",
    "    pred_data['year'] = pred_data['Date'].dt.year\n",
    "    \n",
    "    # One-hot encode categorical columns\n",
    "    categorical_cols = pred_data.select_dtypes(include=['object']).columns\n",
    "    pred_data = pd.get_dummies(pred_data, columns=categorical_cols)\n",
    "    \n",
    "    # Ensure all columns from training data exist in the prediction data\n",
    "    for col in X.columns:\n",
    "        if col not in pred_data.columns:\n",
    "            pred_data[col] = 0\n",
    "    \n",
    "    # Select only the columns used during training\n",
    "    pred_features = pred_data[X.columns]\n",
    "    \n",
    "    # Make prediction\n",
    "    predicted_price = rf_model.predict(pred_features)[0]\n",
    "    \n",
    "    return predicted_price\n",
    "\n",
    "# Load real data from 2023\n",
    "real_data = pd.read_csv(\"../data/raw/Avocado_HassAvocadoBoard_20152023v1.0.1.csv\")\n",
    "real_data['Date'] = pd.to_datetime(real_data['Date'])\n",
    "real_data_2023 = real_data[real_data['Date'].dt.year == 2023]\n",
    "\n",
    "# Select some sample dates and regions for comparison\n",
    "sample_dates = ['2023-01-01', '2023-06-15', '2023-12-31']\n",
    "regions = ['Chicago', 'LosAngeles', 'NewYork', 'SanFrancisco', 'Seattle']\n",
    "\n",
    "print(\"Price Comparison: Predicted vs Actual (2023)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Date':<12} {'Region':<15} {'Type':<12} {'Predicted':<10} {'Actual':<10} {'Difference':<10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "all_predictions = []\n",
    "all_actuals = []\n",
    "\n",
    "for date in sample_dates:\n",
    "    for region in regions:\n",
    "        # Get real data for this date and region\n",
    "        real_conv = real_data_2023[\n",
    "            (real_data_2023['Date'] == date) & \n",
    "            (real_data_2023['region'] == region) & \n",
    "            (real_data_2023['type'] == 'conventional')\n",
    "        ]['AveragePrice'].mean()\n",
    "        \n",
    "        real_org = real_data_2023[\n",
    "            (real_data_2023['Date'] == date) & \n",
    "            (real_data_2023['region'] == region) & \n",
    "            (real_data_2023['type'] == 'organic')\n",
    "        ]['AveragePrice'].mean()\n",
    "        \n",
    "        # Skip if no real data available\n",
    "        if pd.isna(real_conv) or pd.isna(real_org):\n",
    "            continue\n",
    "        \n",
    "        # Make predictions\n",
    "        pred_conv = predict_avocado_price(\n",
    "            region=region,\n",
    "            date_str=date,\n",
    "            avocado_type='conventional',\n",
    "            total_volume=50000,\n",
    "            plu4225=25000\n",
    "        )\n",
    "        \n",
    "        pred_org = predict_avocado_price(\n",
    "            region=region,\n",
    "            date_str=date,\n",
    "            avocado_type='organic',\n",
    "            total_volume=10000,\n",
    "            plu4225=5000\n",
    "        )\n",
    "        \n",
    "        # Store predictions and actuals for metrics\n",
    "        all_predictions.extend([pred_conv, pred_org])\n",
    "        all_actuals.extend([real_conv, real_org])\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"{date:<12} {region:<15} {'Conv':<12} ${pred_conv:<9.2f} ${real_conv:<9.2f} ${pred_conv-real_conv:<9.2f}\")\n",
    "        print(f\"{'':<12} {region:<15} {'Org':<12} ${pred_org:<9.2f} ${real_org:<9.2f} ${pred_org-real_org:<9.2f}\")\n",
    "\n",
    "# Calculate and print overall statistics\n",
    "print(\"\\nModel Performance Statistics (2023)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Convert to numpy arrays and remove any remaining NaN values\n",
    "all_predictions = np.array(all_predictions)\n",
    "all_actuals = np.array(all_actuals)\n",
    "valid_mask = ~(np.isnan(all_predictions) | np.isnan(all_actuals))\n",
    "all_predictions = all_predictions[valid_mask]\n",
    "all_actuals = all_actuals[valid_mask]\n",
    "\n",
    "# Calculate metrics\n",
    "mae = np.mean(np.abs(all_predictions - all_actuals))\n",
    "rmse = np.sqrt(np.mean((all_predictions - all_actuals)**2))\n",
    "r2 = r2_score(all_actuals, all_predictions)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): ${mae:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): ${rmse:.2f}\")\n",
    "print(f\"R-squared Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final evaluation compares our Random Forest model's predictions against actual 2023 avocado prices across five major markets. Despite achieving an exceptional R² of 0.9261 on the test split, performance on completely new 2023 data shows a modest decline to R² = 0.8097, with MAE of $0.15 and RMSE of $0.18.\n",
    "\n",
    "This performance difference between test data and 2023 predictions is expected and reveals important insights:\n",
    "\n",
    "1. The model maintains strong predictive power (R² > 0.80) on genuinely new data, confirming its robustness\n",
    "2. Prediction accuracy varies by market - Chicago conventional shows the largest error ($0.40), while Chicago organic predictions are remarkably accurate ($0.02 difference)\n",
    "3. The model generally predicts organic prices more accurately than conventional ones across regions\n",
    "4. Some regions (like San Francisco) show consistently positive errors, suggesting potential regional pricing trend shifts in 2023\n",
    "\n",
    "The slight performance drop on 2023 data likely stems from market dynamics not present in training data, such as 2023's unique inflation conditions or supply chain adjustments. This real-world validation confirms the model's practical utility while highlighting the importance of periodic retraining to adapt to evolving market conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Implementation for Advanced Avocado Price Prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets trains and evaluates an XGBoost Regressor model for avocado price prediction, using early stopping to optimize the number of boosting rounds, and compares its performance metrics (MAE, RMSE, R²) against other models like Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Define XGBoost with improved parameters\n",
    "xgb_model = XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=7,\n",
    "    min_child_weight=1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    gamma=0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"\\nTraining XGBoost with tuned parameters...\")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
    "rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "\n",
    "print(\"\\nImproved XGBoost Metrics:\")\n",
    "print(f\"  Mean Absolute Error (MAE): {mae_xgb:.4f}\")\n",
    "print(f\"  Root Mean Squared Error (RMSE): {rmse_xgb:.4f}\")\n",
    "print(f\"  R-squared (R²): {r2_xgb:.4f}\")\n",
    "\n",
    "# Feature importance for XGBoost\n",
    "feature_importance_xgb = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': xgb_model.feature_importances_\n",
    "})\n",
    "print(\"\\nTop 10 Most Important Features (XGBoost):\")\n",
    "print(feature_importance_xgb.sort_values(by='Importance', ascending=False).head(10))\n",
    "\n",
    "# Final Comparison Table\n",
    "print(\"\\n===== Model Comparison =====\")\n",
    "print(f\"{'Model':<15}|{'MAE':<10}|{'RMSE':<10}|{'R²':<10}\")\n",
    "print(f\"{'-'*15}|{'-'*10}|{'-'*10}|{'-'*10}\")\n",
    "print(f\"{'Linear Reg':<15}|{mae_lr:<10.4f}|{rmse_lr:<10.4f}|{r2_lr:<10.4f}\")\n",
    "print(f\"{'Random Forest':<15}|{mae_rf:<10.4f}|{rmse_rf:<10.4f}|{r2_rf:<10.4f}\")\n",
    "print(f\"{'XGBoost':<15}|{mae_xgb:<10.4f}|{rmse_xgb:<10.4f}|{r2_xgb:<10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Performance Analysis\n",
    "\n",
    "The XGBoost model delivers strong predictive performance with an R² of 0.8564, significantly outperforming linear regression (R² = 0.4532) but falling short of Random Forest's exceptional accuracy (R² = 0.9261). With an MAE of $0.1112 and RMSE of $0.1472, XGBoost produces reasonably precise predictions while striking a balance between complexity and generalization.\n",
    "\n",
    "Feature importance analysis shows a pattern consistent with other models - avocado type (organic vs. conventional) dominates predictive power at 56.6%, confirming its fundamental role in price determination. Unlike Random Forest, XGBoost places greater emphasis on regional factors, with premium markets (Hartford/Springfield, San Francisco, Seattle, New York) featuring prominently in the top influential variables rather than temporal or volume features.\n",
    "\n",
    "While XGBoost doesn't achieve Random Forest's exceptional accuracy on the test set, its different feature importance profile offers complementary insights about regional price drivers. The model's strong but slightly lower performance might indicate better generalization to unseen data scenarios, potentially making it more robust for long-term forecasting despite the higher error metrics on the current test set. XGBoost's efficient handling of large datasets and regularization capabilities make it a valuable alternative model for production deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather-Enhanced Random Forest Model for Avocado Price Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load avocado data\n",
    "df = pd.read_csv(\"../data/processed/cleaned_avocado_data.csv\")\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "\n",
    "# Load and prepare weather data\n",
    "weather_df = pd.read_csv(\"../data/external/weather_data.csv\")\n",
    "weather_df[\"date\"] = pd.to_datetime(weather_df[\"date\"])\n",
    "weather_df = weather_df.sort_values('date')\n",
    "weather_df = weather_df.rename(columns={'date': 'Date'})\n",
    "\n",
    "# Merge the datasets\n",
    "merged_df = pd.merge(df, weather_df, on=['Date', 'region'], how='left')\n",
    "\n",
    "# Basic preprocessing\n",
    "merged_df[\"type\"] = merged_df[\"type\"].map({\"conventional\": 0, \"organic\": 1})\n",
    "\n",
    "# Extract month from Date\n",
    "merged_df['month'] = merged_df['Date'].dt.month\n",
    "\n",
    "# One-hot encode only region (not Season)\n",
    "merged_df = pd.get_dummies(merged_df, columns=[\"region\"])\n",
    "\n",
    "# Drop the Season column if it exists\n",
    "if 'Season' in merged_df.columns:\n",
    "    merged_df = merged_df.drop(columns=['Season'])\n",
    "\n",
    "# Prepare features and target\n",
    "X = merged_df.drop(columns=[\"Date\", \"AveragePrice\", \"plu4046\", \"plu4225\", \"plu4770\"])\n",
    "y = merged_df[\"AveragePrice\"]\n",
    "\n",
    "# Print shape to verify merge\n",
    "print(\"Shape after merge:\", merged_df.shape)\n",
    "print(\"\\nColumns:\", X.columns.tolist())\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Random Forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=15,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"\\nTraining Random Forest...\")\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
    "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "print(\"\\nRandom Forest Metrics (with weather):\")\n",
    "print(f\"  Mean Absolute Error (MAE): {mae_rf:.4f}\")\n",
    "print(f\"  Root Mean Squared Error (RMSE): {rmse_rf:.4f}\")\n",
    "print(f\"  R-squared (R²): {r2_rf:.4f}\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "})\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(feature_importance.sort_values(by='Importance', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather-Enhanced Model Performance Analysis\n",
    "\n",
    "The integration of weather variables with avocado market data yields a Random Forest model with R² of 0.7488, MAE of $0.1500, and RMSE of $0.1947. Interestingly, this combined model shows lower predictive performance than the market-data-only Random Forest model (R² of 0.9261), suggesting that the addition of weather features introduces complexity without commensurate predictive gains.\n",
    "\n",
    "Feature importance analysis reveals that despite the expanded feature set, type remains the dominant predictor (57.3%), followed by volume metrics (TotalVolume at 5.2%, TotalBags at 4.7%) and temporal factors (month at 4.5%). Weather variables do not appear among the top 10 features, indicating they provide limited incremental predictive value for avocado prices. Regional indicators for premium markets (Hartford/Springfield, San Francisco, Seattle) maintain their significance even with weather data included.\n",
    "\n",
    "The performance decrease with weather data inclusion may result from several factors: (1) potential multicollinearity between weather and seasonal features, (2) noise introduction that dilutes the signal from stronger predictors, or (3) weather impacts being already implicitly captured through seasonal and regional variables. This result challenges the initial hypothesis that explicit weather data would enhance prediction accuracy, suggesting that market factors and seasonality provide sufficient information for optimal avocado price forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest with Weather Data Integration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Random Forest with weather data\n",
    "print(\"\\nTraining Random Forest with weather data...\")\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
    "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "print(\"\\nRandom Forest Metrics (with weather):\")\n",
    "print(f\"  Mean Absolute Error (MAE): {mae_rf:.4f}\")\n",
    "print(f\"  Root Mean Squared Error (RMSE): {rmse_rf:.4f}\")\n",
    "print(f\"  R-squared (R²): {r2_rf:.4f}\")\n",
    "\n",
    "# Feature importance for Random Forest\n",
    "feature_importance_rf = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "})\n",
    "print(\"\\nTop 10 Most Important Features (Random Forest):\")\n",
    "print(feature_importance_rf.sort_values(by='Importance', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weather-enhanced Random Forest model achieves an R² of 0.7488 with MAE of $0.1500 and RMSE of $0.1947, representing a substantial decline from the market-data-only Random Forest (R² of 0.9261). Feature importance remains dominated by avocado type (57.3%), followed by volume metrics (TotalVolume 5.2%, TotalBags 4.7%) and month (4.5%).\n",
    "\n",
    "Surprisingly, no weather variables appear among the top 10 predictors despite the expanded feature set. This suggests weather factors may be (1) already implicitly captured through seasonal and regional variables, (2) introducing noise that obscures stronger market signals, or (3) affecting supply more than retail prices.\n",
    "\n",
    "The performance degradation indicates that simple models with carefully selected features can outperform more complex models with additional variables. For avocado price forecasting, the market fundamentals (type, volume, season, region) appear to provide optimal predictive power without the need for explicit weather data. This finding has practical implications for model deployment, suggesting a more efficient model that requires less data collection and processing could deliver superior results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
